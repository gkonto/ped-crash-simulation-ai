{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Crash Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_reader import DatasetFromFiles, DatasetReaderCSV\n",
    "from pathlib import Path\n",
    "from dataset_preprocessor import DatasetPreprocessor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import utilities\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export simulation datasets from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:\\\\Users\\geork\\projects\\AIThesis\\datasets\\\\20240510\\mlres\")\n",
    "out_path = Path(\"C:\\\\Users\\geork\\projects\\AIThesis\\src\\\\datasets\\\\crash_simulation_no_timeseries.csv\")\n",
    "r = DatasetFromFiles(path)\n",
    "r.setOutputPath(out_path, True)\n",
    "r.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:\\\\Users\\geork\\projects\\AIThesis\\datasets\\\\20240510\\mlres\")\n",
    "out_path = Path(\"C:\\\\Users\\geork\\projects\\AIThesis\\src\\\\datasets\\\\crash_simulation_timeseries.csv\")\n",
    "r = DatasetFromFiles(path)\n",
    "r.setTimeSeriesLabel(\"Head_X_Coordinate\")\n",
    "r.setTimeSeriesLabel(\"Head_Y_Coordinate\")\n",
    "r.setTimeSeriesLabel(\"Head_Z_Coordinate\")\n",
    "r.setTimeSeriesLabel(\"Sternum_X_Coordinate\")\n",
    "r.setTimeSeriesLabel(\"Sternum_Y_Coordinate\")\n",
    "r.setTimeSeriesLabel(\"Sternum_Z_Coordinate\")\n",
    "r.setOutputPath(out_path, True)\n",
    "r.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert csv file to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:\\\\Users\\geork\\projects\\AIThesis\\src\\datasets\\crash_simulation_no_timeseries.csv\")\n",
    "reader = DatasetReaderCSV(path)\n",
    "\n",
    "preprocessor = DatasetPreprocessor()\n",
    "preprocessor.setReader(reader)\n",
    "reader.read()\n",
    "df = reader.convert_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the 'Position' column from the DataFrame\n",
    "utilities.to_scrollable_table(df.drop(columns=[\"Position\"], errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df.describe()\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car Attributes: Profiles - Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'Rotation' column and sort by label\n",
    "counts_translation = df['CarProfile'].value_counts().sort_index()\n",
    "counts_position = df[\"Velocity\"].value_counts().sort_index()\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the first bar chart\n",
    "counts_translation.plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('CarProfile')\n",
    "ax1.set_xlabel('CarProfile')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)  # Rotate x labels for better readability\n",
    "\n",
    "# Plot the second bar chart\n",
    "counts_position.plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Velocity')\n",
    "ax2.set_xlabel('Velocity')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=0)  # Rotate x labels for better readability\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pedestration Attributes: Translation - Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rotation labels to integers if they are not already\n",
    "df['Translation'] = df['Translation'].astype(int)\n",
    "df[\"Rotation\"] = df[\"Rotation\"].astype(int)\n",
    "    \n",
    "# Count the occurrences of each unique value in the 'Rotation' column and sort by label\n",
    "counts_translation = df['Translation'].value_counts().sort_index()\n",
    "counts_position = df[\"Rotation\"].value_counts().sort_index()\n",
    "\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot the first bar chart\n",
    "counts_translation.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Translation')\n",
    "ax1.set_xlabel('Translation')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)  # Rotate x labels for better readability\n",
    "\n",
    "# Plot the second bar chart\n",
    "counts_position.plot(kind='bar', ax=ax2, color='lightgreen')\n",
    "ax2.set_title('Rotation')\n",
    "ax2.set_xlabel('Rotation')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.tick_params(axis='x', rotation=0)  # Rotate x labels for better readability\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible target value: HIC15_max / HIC36_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "\n",
    "# Vertical lines plot for 'HIC15_max'\n",
    "axes[0].vlines(df.index, ymin=0, ymax=df['HIC15_max'], color='blue', alpha=0.5)\n",
    "axes[0].set_title('HIC15_max')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('HIC15_max')\n",
    "\n",
    "# Vertical lines plot for 'HIC36_max'\n",
    "axes[1].vlines(df.index, ymin=0, ymax=df['HIC36_max'], color='green', alpha=0.5)\n",
    "axes[1].set_title('HIC36_max')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('HIC36_max')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head XYZ Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "# Vertical lines plot for 'HIC15_max'\n",
    "axes[0].vlines(df.index, ymin=0, ymax=df['Head_X_Acceleration_abs_max'], color='blue', alpha=0.5)\n",
    "axes[0].set_title('Head_X_Acceleration_abs_max')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Head_X_Acceleration_abs_max')\n",
    "\n",
    "# Vertical lines plot for 'HIC36_max'\n",
    "axes[1].vlines(df.index, ymin=0, ymax=df['Head_Y_Acceleration_abs_max'], color='green', alpha=0.5)\n",
    "axes[1].set_title('Head_Y_Acceleration_abs_max')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('Head_Y_Acceleration_abs_max')\n",
    "\n",
    "# Vertical lines plot for 'HIC36_max'\n",
    "axes[2].vlines(df.index, ymin=0, ymax=df['Head_Z_Acceleration_abs_max'], color='red', alpha=0.5)\n",
    "axes[2].set_title('Head_Z_Acceleration_abs_max')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Head_Z_Acceleration_abs_max')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Injury Damage(BrIC) - Chest_Resultant_Acceleration_max - Chest_Resultant_Acceleration_CLIP3ms_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "\n",
    "# Vertical lines plot for 'HIC15_max'\n",
    "axes[0].vlines(df.index, ymin=0, ymax=df['BrIC_abs_max'], color='blue', alpha=0.5)\n",
    "axes[0].set_title('BrIC_abs_max')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('BrIC_abs_max')\n",
    "\n",
    "# Vertical lines plot for 'HIC36_max'\n",
    "axes[1].vlines(df.index, ymin=0, ymax=df['Chest_Resultant_Acceleration_max'], color='green', alpha=0.5)\n",
    "axes[1].set_title('Chest_Resultant_Acceleration_max')\n",
    "axes[1].set_xlabel('Index')\n",
    "axes[1].set_ylabel('Chest_Resultant_Acceleration_max')\n",
    "\n",
    "# Vertical lines plot for 'HIC36_max'\n",
    "axes[2].vlines(df.index, ymin=0, ymax=df['Chest_Resultant_Acceleration_CLIP3ms_max'], color='red', alpha=0.5)\n",
    "axes[2].set_title('Chest_Resultant_Acceleration_CLIP3ms_max')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Chest_Resultant_Acceleration_CLIP3ms_max')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification: HIC15_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HIC15 to binary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 800\n",
    "over_thres  = (df[\"HIC15_max\"] > threshold).sum()\n",
    "under_thres = (df[\"HIC15_max\"] <= threshold).sum()\n",
    "\n",
    "print(\"Entries over the threshold:\", over_thres)\n",
    "print(\"Entries under the threshold:\", under_thres)\n",
    "\n",
    "# Data for the bar chart\n",
    "categories = ['Over Threshold', 'Under Threshold']\n",
    "counts = [over_thres, under_thres]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(categories, counts, color=['blue', 'green'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Entries Over and Under Threshold: 800')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_features = [\"Id\", \"Position\", \"Path\",\n",
    "                      \"HIC36_max\", \n",
    "                      \"Head_Z_Acceleration_abs_max\", \"Head_X_Acceleration_abs_max\", \"Head_Y_Acceleration_abs_max\",\n",
    "                      \"BrIC_abs_max\", \n",
    "                      \"Chest_Resultant_Acceleration_max\", \"Chest_Resultant_Acceleration_CLIP3ms_max\"]\n",
    "# Remove the unwanted columns\n",
    "dfn = df.drop(columns=to_remove_features)\n",
    "\n",
    "# Create a new target column for the classification\n",
    "dfn[\"HIC15_over_800\"] = (dfn[\"HIC15_max\"] > 800).astype(int)\n",
    "dfn = dfn.drop(columns=[\"HIC15_max\"])\n",
    "\n",
    "# Use pd.get_dummies for encoding\n",
    "dfn = pd.get_dummies(dfn, columns=[\"CarProfile\"], drop_first=False)\n",
    "print(dfn.dtypes)\n",
    "print(dfn.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = dfn.drop(columns=[\"HIC15_over_800\"])\n",
    "y = dfn[\"HIC15_over_800\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Print value counts for the last four columns\n",
    "columns_to_check = ['CarProfile_FCR', 'CarProfile_MPV', 'CarProfile_RDS', 'CarProfile_SUV']\n",
    "for column in columns_to_check:\n",
    "    print(f\"Value counts for {column}:\")\n",
    "    print(X_test[column].value_counts())\n",
    "    print()\n",
    "# Step 6: Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Build and train the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot the model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIC15 regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_features = [\"Unnamed: 0\", \"Position\", \"Path\",\n",
    "                      \"HIC36_max\", \n",
    "                      \"Head_Z_Acceleration_abs_max\", \"Head_X_Acceleration_abs_max\", \"Head_Y_Acceleration_abs_max\",\n",
    "                      \"BrIC_abs_max\", \n",
    "                      \"Chest_Resultant_Acceleration_max\", \"Chest_Resultant_Acceleration_CLIP3ms_max\"]\n",
    "# Remove the unwanted columns\n",
    "dfn_regr = df.drop(columns=to_remove_features)\n",
    "\n",
    "# Use pd.get_dummies for encoding\n",
    "dfn_regr = pd.get_dummies(dfn_regr, columns=[\"CarProfile\"], drop_first=False)\n",
    "print(dfn_regr.dtypes)\n",
    "print(dfn_regr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = dfn_regr.drop(columns=[\"HIC15_max\"])\n",
    "y = dfn_regr[\"HIC15_max\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Print value counts for the last four columns\n",
    "columns_to_check = ['CarProfile_FCR', 'CarProfile_MPV', 'CarProfile_RDS', 'CarProfile_SUV']\n",
    "for column in columns_to_check:\n",
    "    print(f\"Value counts for {column}:\")\n",
    "    print(X_test[column].value_counts())\n",
    "    print()\n",
    "# Step 6: Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression attempt #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test MAE: {mae}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression attempt #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Early stopping\n",
    "#early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Train the model\n",
    "#history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1, callbacks=[early_stop])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test MSE: {loss}, Test MAE: {mae}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's performance, as indicated by the high loss and MAE values, suggests that it may be underfitting or the dataset may have issues that need addressing. Here are several strategies to improve your model:\n",
    "\n",
    "1. Data Preprocessing\n",
    "- Feature Engineering: Create new features that might better represent the underlying data patterns.\n",
    "- Scaling: Ensure all features are properly scaled.\n",
    "- Outlier Removal: Remove or cap outliers in the dataset.\n",
    "- Feature Selection: Ensure only the most relevant features are included.\n",
    "2. Model Architecture\n",
    "- Increase Complexity: Add more layers or neurons to your model.\n",
    "- Activation Functions: Try different activation functions such as LeakyReLU or ELU.\n",
    "- Regularization: Adjust regularization parameters.\n",
    "3. Hyperparameter Tuning\n",
    "- Use tools like GridSearchCV or RandomizedSearchCV to find the best hyperparameters.\n",
    "4. Training Techniques\n",
    "- Early Stopping: Use EarlyStopping with a more patient threshold.\n",
    "- Learning Rate Scheduling: Adjust learning rates dynamically during training.\n",
    "- Batch Normalization: Add batch normalization layers.\n",
    "5. Visualizing and Debugging\n",
    "- Residual Analysis: Analyze the residuals to understand model errors.\n",
    "- Cross-Validation: Use cross-validation to ensure robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression attempt #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes and Additions:\n",
    "- Batch Normalization: Added after each Dense layer to help stabilize and speed up training.\n",
    "- Dropout: Added to prevent overfitting.\n",
    "- Early Stopping: More patient with a restore best weights option.\n",
    "- Learning Rate Scheduling: Reduce learning rate if validation loss plateaus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_split=0.2, batch_size=32, verbose=1, callbacks=[early_stop, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test MSE: {loss}, Test MAE: {mae}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming a Regression Problem into a Balanced Classification Task using Quantile Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the target variable HIC15_max into bins with an approximately equal number of items is a method called quantile binning. <br>\n",
    "This ensures that each bin contains roughly the same number of samples, regardless of the actual value distribution.<br> This method can help address issues with skewed distributions and provide more balanced classes for classification tasks.\n",
    "\n",
    "Here's how you can implement quantile binning in Python:\n",
    "\n",
    "Use pd.qcut to create quantile bins.\n",
    "Update the target variable and re-split the dataset.\n",
    "Proceed with training a classification model.<br>\n",
    "Here's a step-by-step guide and the corresponding code:\n",
    "\n",
    "Step-by-Step Guide\n",
    "Create Quantile Bins:\n",
    "\n",
    "Use pd.qcut to split HIC15_max into a specified number of quantiles, ensuring each bin has an approximately equal number of samples.<br>\n",
    "Modify the Target Variable:\n",
    "\n",
    "Replace the continuous HIC15_max values with the new quantile bins.\n",
    "Split the Data and Train a Classification Model:\n",
    "\n",
    "Split the data into training and testing sets.\n",
    "Standardize the features if necessary.\n",
    "Train a classification model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Quantile Bins: The pd.qcut function automatically determines the bin edges so that each bin contains approximately the same number of observations. <br>Adjust number_of_bins according to your needs.\n",
    "Labels: Assign meaningful labels to the bins, such as Q1, Q2, etc.\n",
    "Model Selection and Evaluation: Different models might perform better on this transformed problem. Experiment with various classifiers and hyperparameters.\n",
    "By using quantile binning, you ensure that your classes are balanced, which can lead to better performance and more reliable evaluation metrics for your classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfn_regr is your DataFrame\n",
    "# Check the distribution of the target variable\n",
    "plt.hist(dfn_regr['HIC15_max'], bins=50)\n",
    "plt.xlabel('HIC15_max')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of HIC15_max')\n",
    "plt.show()\n",
    "\n",
    "# Create quantile bins\n",
    "number_of_bins = 8  # You can adjust the number of bins as needed\n",
    "y_binned, bin_edges = pd.qcut(dfn_regr['HIC15_max'], q=number_of_bins, labels=[f'Q{i+1}' for i in range(number_of_bins)], retbins=True)\n",
    "\n",
    "# Print the ranges of the bins\n",
    "print(\"Ranges for each quantile bin:\")\n",
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Q{i+1}: {bin_edges[i]} to {bin_edges[i+1]}\")\n",
    "\n",
    "# Replace the continuous target variable with the new binned variable\n",
    "X = dfn_regr.drop(columns=[\"HIC15_max\"])\n",
    "y = y_binned\n",
    "print(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a classification model\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfn_regr is your DataFrame\n",
    "# Check the distribution of the target variable\n",
    "plt.hist(dfn_regr['HIC15_max'], bins=50)\n",
    "plt.xlabel('HIC15_max')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of HIC15_max')\n",
    "plt.show()\n",
    "\n",
    "# Create quantile bins\n",
    "number_of_bins = 3  # You can adjust the number of bins as needed\n",
    "y_binned, bin_edges = pd.qcut(dfn_regr['HIC15_max'], q=number_of_bins, labels=[f'Q{i+1}' for i in range(number_of_bins)], retbins=True)\n",
    "\n",
    "# Print the ranges of the bins\n",
    "print(\"Ranges for each quantile bin:\")\n",
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Q{i+1}: {bin_edges[i]} to {bin_edges[i+1]}\")\n",
    "\n",
    "# Replace the continuous target variable with the new binned variable\n",
    "X = dfn_regr.drop(columns=[\"HIC15_max\"])\n",
    "y = y_binned\n",
    "print(y)\n",
    "\n",
    "# Convert categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target variable to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=number_of_bins)\n",
    "y_test_cat = to_categorical(y_test, num_classes=number_of_bins)\n",
    "\n",
    "# Define the deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_bins, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_cat, epochs=50, batch_size=32, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cat = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_cat, axis=1)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfn_regr is your DataFrame\n",
    "# Check the distribution of the target variable\n",
    "plt.hist(dfn_regr['HIC15_max'], bins=50)\n",
    "plt.xlabel('HIC15_max')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of HIC15_max')\n",
    "plt.show()\n",
    "\n",
    "# Create quantile bins\n",
    "number_of_bins = 8  # You can adjust the number of bins as needed\n",
    "y_binned, bin_edges = pd.qcut(dfn_regr['HIC15_max'], q=number_of_bins, labels=[f'Q{i+1}' for i in range(number_of_bins)], retbins=True)\n",
    "\n",
    "# Print the ranges of the bins\n",
    "print(\"Ranges for each quantile bin:\")\n",
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Q{i+1}: {bin_edges[i]} to {bin_edges[i+1]}\")\n",
    "\n",
    "# Replace the continuous target variable with the new binned variable\n",
    "X = dfn_regr.drop(columns=[\"HIC15_max\"])\n",
    "y = y_binned\n",
    "\n",
    "# Convert categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target variable to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=number_of_bins)\n",
    "y_test_cat = to_categorical(y_test, num_classes=number_of_bins)\n",
    "\n",
    "# Define the deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_bins, activation='softmax'))\n",
    "\n",
    "# Compile the model with a different learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_cat, epochs=200, batch_size=32, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cat = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_cat, axis=1)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfn_regr is your DataFrame\n",
    "# Check the distribution of the target variable\n",
    "plt.hist(dfn_regr['HIC15_max'], bins=50)\n",
    "plt.xlabel('HIC15_max')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of HIC15_max')\n",
    "plt.show()\n",
    "\n",
    "# Create quantile bins\n",
    "number_of_bins = 8  # You can adjust the number of bins as needed\n",
    "y_binned, bin_edges = pd.qcut(dfn_regr['HIC15_max'], q=number_of_bins, labels=[f'Q{i+1}' for i in range(number_of_bins)], retbins=True)\n",
    "\n",
    "# Print the ranges of the bins\n",
    "print(\"Ranges for each quantile bin:\")\n",
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Q{i+1}: {bin_edges[i]} to {bin_edges[i+1]}\")\n",
    "\n",
    "# Replace the continuous target variable with the new binned variable\n",
    "X = dfn_regr.drop(columns=[\"HIC15_max\"])\n",
    "y = y_binned\n",
    "\n",
    "# Convert categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize/normalize the data if necessary\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target variable to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=number_of_bins)\n",
    "y_test_cat = to_categorical(y_test, num_classes=number_of_bins)\n",
    "\n",
    "# Define the deep learning model with L2 regularization\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_bins, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_cat, epochs=100, batch_size=32, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cat = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_cat, axis=1)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining whether you need more data depends on several factors related to your current model's performance, the complexity of the problem, and the quality of the data. Here are some key considerations to help you decide if acquiring more data would be beneficial:\n",
    "\n",
    "Indicators That You Might Need More Data\n",
    "Validation Accuracy Plateau:\n",
    "\n",
    "If your validation accuracy has plateaued and is not improving significantly despite different training techniques, this might indicate that the model has learned as much as it can from the current dataset and additional data could help.\n",
    "Overfitting Signs:\n",
    "\n",
    "If your model's training accuracy is much higher than the validation accuracy, this indicates overfitting. More data can help the model generalize better.\n",
    "High Variance in Performance:\n",
    "\n",
    "If there is high variability in model performance across different validation sets, this could indicate that your model is not seeing enough examples to learn robust patterns.\n",
    "Complexity of the Problem:\n",
    "\n",
    "Complex problems, especially those involving high-dimensional data or numerous classes, often benefit from larger datasets to capture the necessary variability and nuances.\n",
    "Current Performance Analysis\n",
    "Current Accuracy:\n",
    "\n",
    "Your model's accuracy on the test set is 49%, which suggests it is not performing optimally. While this indicates there is room for improvement, it doesn't necessarily mean more data is the only solution.\n",
    "Learning Curve Analysis:\n",
    "\n",
    "Plotting learning curves (accuracy and loss for both training and validation sets over epochs) can help you understand if more data is needed. If the validation curve shows a significant gap from the training curve that doesn't narrow with more epochs, more data could help.\n",
    "Strategies to Improve Without More Data\n",
    "Before deciding to gather more data, consider these strategies to improve model performance:\n",
    "\n",
    "Data Augmentation:\n",
    "\n",
    "Generate synthetic data through augmentation techniques such as rotations, translations, and noise addition if your data is image-based. This can effectively increase the dataset size and variability without acquiring new data.\n",
    "Regularization Techniques:\n",
    "\n",
    "Use dropout, L2 regularization, or batch normalization to prevent overfitting and help the model generalize better.\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Experiment with different hyperparameters like learning rate, batch size, and network architecture to find the optimal settings for your model.\n",
    "Feature Engineering:\n",
    "\n",
    "Improve the quality of your features or extract new, more relevant features from your existing data.\n",
    "Cross-Validation:\n",
    "\n",
    "Use k-fold cross-validation to ensure your model's performance is robust and not due to random chance in train-test splits.\n",
    "Conclusion\n",
    "While more data can often help improve model performance, it is not always the first or most efficient solution. Analyze your current performance, employ advanced techniques to optimize the existing model, and use learning curves to make an informed decision. If after these improvements your model still shows signs of underperformance and overfitting, acquiring more data would likely be beneficial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
